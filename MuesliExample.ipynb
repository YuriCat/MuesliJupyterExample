{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment:\n",
    "# pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of simple game: Tic-Tac-Toe\n",
    "# You can change this to another two-player game.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "BLACK, WHITE = 1, -1  # first turn or second turn player\n",
    "\n",
    "class State:\n",
    "    '''Board implementation of Tic-Tac-Toe'''\n",
    "    X, Y = 'ABC',  '123'\n",
    "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3)) # (x, y)\n",
    "        self.color = 1\n",
    "        self.win_color = 0\n",
    "        self.record = []\n",
    "\n",
    "    def action2str(self, a):\n",
    "        return self.X[a // 3] + self.Y[a % 3]\n",
    "\n",
    "    def str2action(self, s):\n",
    "        return self.X.find(s[0]) * 3 + self.Y.find(s[1])\n",
    "\n",
    "    def record_string(self):\n",
    "        return ' '.join([self.action2str(a) for a in self.record])\n",
    "\n",
    "    def __str__(self):\n",
    "        # output board.\n",
    "        s = '   ' + ' '.join(self.Y) + '\\n'\n",
    "        for i in range(3):\n",
    "            s += self.X[i] + ' ' + ' '.join([self.C[self.board[i, j]] for j in range(3)]) + '\\n'\n",
    "        s += 'record = ' + self.record_string()\n",
    "        return s\n",
    "\n",
    "    def play(self, action):\n",
    "        # state transition function\n",
    "        # action is position inerger (0~8) or string representation of action sequence\n",
    "        if isinstance(action, str):\n",
    "            for astr in action.split():\n",
    "                self.play(self.str2action(astr))\n",
    "            return self\n",
    "\n",
    "        x, y = action // 3, action % 3\n",
    "        self.board[x, y] = self.color\n",
    "\n",
    "        # check whether 3 stones are on the line\n",
    "        if self.board[x, :].sum() == 3 * self.color \\\n",
    "          or self.board[:, y].sum() == 3 * self.color \\\n",
    "          or (x == y and np.diag(self.board, k=0).sum() == 3 * self.color) \\\n",
    "          or (x == 2 - y and np.diag(self.board[::-1,:], k=0).sum() == 3 * self.color):\n",
    "            self.win_color = self.color\n",
    "\n",
    "        self.color = -self.color\n",
    "        self.record.append(action)\n",
    "        return self\n",
    "\n",
    "    def terminal(self):\n",
    "        # terminal state check\n",
    "        return self.win_color != 0 or len(self.record) == 3 * 3\n",
    "\n",
    "    def terminal_reward(self):\n",
    "        # terminal reward \n",
    "        return self.win_color\n",
    "\n",
    "    def action_length(self):\n",
    "        return 3 * 3\n",
    "    \n",
    "    def legal_actions(self):\n",
    "        # list of legal actions on each state\n",
    "        return [a for a in range(3 * 3) if self.board[a // 3, a % 3] == 0]\n",
    "\n",
    "    def feature(self):\n",
    "        # input tensor for neural net (state)\n",
    "        return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
    "\n",
    "    def action_feature(self, action):\n",
    "        # input tensor for neural net (action)\n",
    "        a = np.zeros((1, 3, 3), dtype=np.float32)\n",
    "        a[0, action // 3, action % 3] = 1\n",
    "        return a\n",
    "\n",
    "state = State().play('B1')\n",
    "print(state)\n",
    "print('input feature')\n",
    "print(state.feature())\n",
    "state = State().play('B2 A1 C2')\n",
    "print('input feature')\n",
    "print(state.feature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small neural nets with PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn = None\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(filters1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            h = self.bn(h)\n",
    "        return h\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(filters, filters, 3, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + (self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 16\n",
    "num_blocks = 4\n",
    "\n",
    "class Representation(nn.Module):\n",
    "    ''' Conversion from observation to inner abstract state '''\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
    "\n",
    "        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.layer0(x))\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            rp = self(torch.from_numpy(x).unsqueeze(0))\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Prediction(nn.Module):\n",
    "    ''' Policy and value prediction from inner abstract state '''\n",
    "    def __init__(self, action_shape):\n",
    "        super().__init__()\n",
    "        self.board_size = np.prod(action_shape[1:])\n",
    "        self.action_size = action_shape[0] * self.board_size\n",
    "\n",
    "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.conv_p2 = Conv(4, 1, 1)\n",
    "\n",
    "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
    "\n",
    "    def forward(self, rp):\n",
    "        h_p = F.relu(self.conv_p1(rp))\n",
    "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
    "\n",
    "        h_v = F.relu(self.conv_v(rp))\n",
    "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
    "\n",
    "        # range of value is -1 ~ 1\n",
    "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
    "\n",
    "    def inference(self, rp):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            p, v = self(torch.from_numpy(rp).unsqueeze(0))\n",
    "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    '''Abstract state transition'''\n",
    "    def __init__(self, rp_shape, act_shape):\n",
    "        super().__init__()\n",
    "        self.rp_shape = rp_shape\n",
    "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, rp, a):\n",
    "        h = torch.cat([rp, a], dim=1)\n",
    "        h = self.layer0(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, rp, a):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            rp = self(torch.from_numpy(rp).unsqueeze(0), torch.from_numpy(a).unsqueeze(0))\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''Whole net'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        state = State()\n",
    "        input_shape = state.feature().shape\n",
    "        action_shape = state.action_feature(0).shape\n",
    "        rp_shape = (num_filters, *input_shape[1:])\n",
    "\n",
    "        self.representation = Representation(input_shape)\n",
    "        self.prediction = Prediction(action_shape)\n",
    "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
    "\n",
    "    def predict(self, state0, path):\n",
    "        '''Predict p and v from original state and path'''\n",
    "        outputs = []\n",
    "        x = state0.feature()\n",
    "        rp = self.representation.inference(x)\n",
    "        outputs.append(self.prediction.inference(rp))\n",
    "        for action in path:\n",
    "            a = state0.action_feature(action)\n",
    "            rp = self.dynamics.inference(rp, a)\n",
    "            outputs.append(self.prediction.inference(rp))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_net(net, state):\n",
    "    '''Display policy (p) and value (v)'''\n",
    "    print(state)\n",
    "    p, v = net.predict(state, [])[-1]\n",
    "    print('p = ')\n",
    "    print((p * 1000).astype(int).reshape((-1, *net.representation.input_shape[1:3])))\n",
    "    print('v = ', v)\n",
    "    print()\n",
    "\n",
    "#  Outputs before training\n",
    "show_net(Net(), State())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of neural net\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 100\n",
    "K = 1\n",
    "\n",
    "def gen_target(state, ep):\n",
    "    '''Generate inputs and targets for training'''\n",
    "    # path, reward, observation, action, policy\n",
    "    ep_length = len(ep['feature'])\n",
    "    turn_idx = np.random.randint(ep_length)\n",
    "    \n",
    "    x = ep['feature'][turn_idx]\n",
    "    ps, rs, acts, axs = [], [], [], []\n",
    "    sas, seas, szs = [], [], []\n",
    "    for t in range(turn_idx, turn_idx + K + 1):\n",
    "        if t < ep_length:\n",
    "            p = ep['policy'][t]\n",
    "            a = ep['action'][t]\n",
    "            ax = ep['action_feature'][t]\n",
    "            sa = ep['sampled_info'][t]['a']\n",
    "            sea = ep['sampled_info'][t]['exadv']\n",
    "            sz = ep['sampled_info'][t]['z']\n",
    "        else: # state after finishing game\n",
    "            p = np.zeros_like(ep['policy'][-1])\n",
    "            # random action selection\n",
    "            a = np.random.randint(state.action_length())\n",
    "            ax = state.action_feature(a)\n",
    "            sa = np.random.randint(state.action_length(), size=len(sa))\n",
    "            sea = np.ones_like(sea)\n",
    "            sz = np.ones_like(sz)\n",
    "        \n",
    "        rs.append([ep['reward'] if t % 2 == 0 else -ep['reward']])\n",
    "        acts.append([a])\n",
    "        axs.append(ax)\n",
    "        ps.append(p)\n",
    "        sas.append(sa)\n",
    "        seas.append(sea)\n",
    "        szs.append(sz)\n",
    "        \n",
    "    return x, rs, acts, axs, ps, sas, seas, szs\n",
    "\n",
    "def train(episodes, net, opt):\n",
    "    '''Train neural net'''\n",
    "    pg_loss_sum, cmpo_loss_sum, v_loss_sum = 0, 0, 0\n",
    "    net.train()\n",
    "    state = State()\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        targets = [gen_target(state, episodes[np.random.randint(len(episodes))]) for j in range(batch_size)]\n",
    "        x, r, a, ax, p_prior, sa, sea, sz = zip(*targets)\n",
    "        x = torch.from_numpy(np.array(x))\n",
    "        r = torch.from_numpy(np.array(r))\n",
    "        a = torch.from_numpy(np.array(a))\n",
    "        ax = torch.from_numpy(np.array(ax))\n",
    "        p_prior = torch.from_numpy(np.array(p_prior))\n",
    "        sa = torch.from_numpy(np.array(sa))\n",
    "        sea = torch.from_numpy(np.array(sea))\n",
    "        sz = torch.from_numpy(np.array(sz))\n",
    "\n",
    "        # Compute losses for k (+ current) steps\n",
    "        ps, vs = [], []\n",
    "        rp = net.representation(x)\n",
    "        for t in range(K + 1):\n",
    "            p, v = net.prediction(rp)\n",
    "            ps.append(p)\n",
    "            vs.append(v)\n",
    "            rp = net.dynamics(rp, ax[:, t])\n",
    "\n",
    "        cmpo_loss, v_loss = 0, 0\n",
    "        for t in range(K, -1, -1):\n",
    "            cmpo_loss += -torch.mean(sea[:, t] / sz[:, t] * torch.log(ps[t].gather(1, sa[:, t])), dim=1).sum()\n",
    "            v_loss += torch.sum(((vs[t] - r[:, t]) ** 2) / 2)\n",
    "\n",
    "        p_selected = ps[0].gather(1, a[:, 0])\n",
    "        p_selected_prior = p_prior[:, 0].gather(1, a[:, 0])\n",
    "        clipped_rho = torch.clamp(p_selected.detach() / p_selected_prior, 0, 1)\n",
    "        pg_loss = torch.sum(-clipped_rho * torch.log(p_selected) * (r[:, 0] - vs[0]))\n",
    "\n",
    "        pg_loss_sum  += pg_loss.item()\n",
    "        cmpo_loss_sum += cmpo_loss.item() / (K + 1)\n",
    "        v_loss_sum += v_loss.item() / (K + 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (pg_loss + cmpo_loss + v_loss).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    data_count = num_steps * batch_size\n",
    "    print('pg_loss %f cmpo_loss %f v_loss %f' % (pg_loss_sum / data_count, cmpo_loss_sum / data_count, v_loss_sum / data_count))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Battle against random agents\n",
    "\n",
    "def vs_random(net, n=100):\n",
    "    results = {}\n",
    "    for i in range(n):\n",
    "        first_turn = i % 2 == 0\n",
    "        turn = first_turn\n",
    "        state = State()\n",
    "        while not state.terminal():\n",
    "            if turn:\n",
    "                p, _ = net.predict(state, [])[-1]\n",
    "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
    "            else:\n",
    "                action = np.random.choice(state.legal_actions())\n",
    "            state.play(action)\n",
    "            turn = not turn\n",
    "        r = state.terminal_reward() if first_turn else -state.terminal_reward()\n",
    "        results[r] = results.get(r, 0) + 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Main algorithm of Muesli\n",
    "\n",
    "num_games = 5000\n",
    "num_games_one_epoch = 40\n",
    "num_sampled_actions = 10\n",
    "simulation_depth = 1\n",
    "\n",
    "C = 1\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.8)\n",
    "\n",
    "# Display battle results\n",
    "vs_random_sum = vs_random(net)\n",
    "print('vs_random   win: %d  draw: %d  lose: %d' %\n",
    "         (vs_random_sum.get(1, 0), vs_random_sum.get(0, 0), vs_random_sum.get(-1, 0)))\n",
    "\n",
    "episodes = []\n",
    "result_distribution = {1: 0, 0: 0, -1: 0}\n",
    "\n",
    "for g in range(num_games):\n",
    "    # Generate one episode\n",
    "    state = State()\n",
    "\n",
    "    features, policies,  selected_actions,  selected_action_features = [], [], [], []\n",
    "    sampled_infos = []\n",
    "    while not state.terminal():\n",
    "        feature = state.feature()\n",
    "        rp_root = net.representation.inference(feature)\n",
    "        p_root, v_root = net.prediction.inference(rp_root)\n",
    "        p_mask = np.zeros_like(p_root)\n",
    "        p_mask[state.legal_actions()] = 1\n",
    "        p_root *= p_mask\n",
    "        p_root /= p_root.sum()\n",
    "        \n",
    "        features.append(feature)\n",
    "        policies.append(p_root)\n",
    "\n",
    "        actions, exadvs = [], []\n",
    "        for i in range(num_sampled_actions):\n",
    "            action = np.random.choice(np.arange(len(p_root)), p=p_root)\n",
    "            actions.append(action)\n",
    "\n",
    "            rp = rp_root\n",
    "            qs = []\n",
    "            for t in range(simulation_depth):\n",
    "                action_feature = state.action_feature(action)\n",
    "                rp = net.dynamics.inference(rp, action_feature)\n",
    "                p, v = net.prediction.inference(rp)\n",
    "                qs.append(-v if t % 2 == 0 else v)\n",
    "                action = np.random.choice(np.arange(len(p)), p=p)\n",
    "\n",
    "            q = np.mean(qs)\n",
    "            exadvs.append(np.exp(np.clip(q - v_root, -C, C)))\n",
    "    \n",
    "        exadv_sum = np.sum(exadvs)\n",
    "        zs = []\n",
    "        for exadv in exadvs:\n",
    "            z = (1 + exadv_sum - exadv) / num_sampled_actions\n",
    "            zs.append(z)\n",
    "        sampled_infos.append({'a': actions, 'q': qs, 'exadv': exadvs, 'z': zs})\n",
    "\n",
    "        # Select action with generated distribution, and then make a transition by that action\n",
    "        selected_action = np.random.choice(np.arange(len(p_root)), p=p_root)\n",
    "        selected_actions.append(selected_action)\n",
    "        selected_action_features.append(state.action_feature(selected_action))\n",
    "        state.play(selected_action)\n",
    "\n",
    "    # reward seen from the first turn player\n",
    "    reward = state.terminal_reward()\n",
    "    result_distribution[reward] += 1\n",
    "    episodes.append({\n",
    "        'feature': features, 'action': selected_actions, \n",
    "        'action_feature': selected_action_features, 'policy': policies,\n",
    "        'reward': reward,\n",
    "        'sampled_info': sampled_infos})\n",
    "\n",
    "    if g % num_games_one_epoch == 0:\n",
    "        print('game ', end='')\n",
    "    print(g, ' ', end='')\n",
    "\n",
    "    # Training of neural net\n",
    "    if (g + 1) % num_games_one_epoch == 0:\n",
    "        # Show the result distributiuon of generated episodes\n",
    "        print('generated = ', sorted(result_distribution.items()))\n",
    "        net = train(episodes, net, optimizer)\n",
    "        vs_random_once = vs_random(net)\n",
    "        print('vs_random   win: %d  draw: %d  lose: %d' %\n",
    "                  (vs_random_once.get(1, 0), vs_random_once.get(0, 0), vs_random_once.get(-1, 0)))\n",
    "        for r, n in vs_random_once.items():\n",
    "            vs_random_sum[r] += n\n",
    "        print('(total)           win: %d  draw: %d  lose: %d ' %\n",
    "                  (vs_random_sum.get(1, 0), vs_random_sum.get(0, 0), vs_random_sum.get(-1, 0)))\n",
    "        #show_net(net, State())\n",
    "        #show_net(net, State().play('A1 C1 A2 C2'))\n",
    "        #show_net(net, State().play('A1 B2 C3 B3 C1'))\n",
    "        #show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
    "        #show_net(net, State().play('B2 A2 A3 C1'))\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outputs from trained net\n",
    "\n",
    "print('initial state')\n",
    "show_net(net, State())\n",
    "\n",
    "print('WIN by put')\n",
    "show_net(net, State().play('A1 C1 A2 C2'))\n",
    "\n",
    "print('LOSE by opponent\\'s double')\n",
    "show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
    "\n",
    "print('WIN through double')\n",
    "show_net(net, State().play('B2 A2 A3 C1'))\n",
    "\n",
    "# hard case: putting on A1 will cause double\n",
    "print('strategic WIN by following double')\n",
    "show_net(net, State().play('B1 A3'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
